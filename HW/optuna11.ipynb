{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e309f615-b907-4598-9b17-be1c8f866d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-26 00:02:53,960] A new study created in memory with name: no-name-fb22f273-f2d7-4160-a5a9-45038e0f1f96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading the train set of COVID19 Dataset (1800 samples found, each dim = 53)\n",
      "Finished reading the dev set of COVID19 Dataset (200 samples found, each dim = 53)\n",
      "Finished reading the test set of COVID19 Dataset (650 samples found, each dim = 53)\n",
      "Finished reading the train set of COVID19 Dataset (1800 samples found, each dim = 53)\n",
      "Finished reading the dev set of COVID19 Dataset (200 samples found, each dim = 53)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-26 00:04:13,845] Trial 0 finished with value: 0.7385624647140503 and parameters: {'lr': 2.1174282210628097e-05, 'batch_size': 27, 'n_epochs': 2379}. Best is trial 0 with value: 0.7385624647140503.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading the train set of COVID19 Dataset (1800 samples found, each dim = 53)\n",
      "Finished reading the dev set of COVID19 Dataset (200 samples found, each dim = 53)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-26 00:05:26,376] Trial 1 finished with value: 0.6833949089050293 and parameters: {'lr': 0.04658269276853471, 'batch_size': 22, 'n_epochs': 2148}. Best is trial 1 with value: 0.6833949089050293.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading the train set of COVID19 Dataset (1800 samples found, each dim = 53)\n",
      "Finished reading the dev set of COVID19 Dataset (200 samples found, each dim = 53)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-26 00:07:23,129] Trial 2 finished with value: 0.8816934823989868 and parameters: {'lr': 0.00032080860602083904, 'batch_size': 72, 'n_epochs': 3400}. Best is trial 1 with value: 0.6833949089050293.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading the train set of COVID19 Dataset (1800 samples found, each dim = 53)\n",
      "Finished reading the dev set of COVID19 Dataset (200 samples found, each dim = 53)\n"
     ]
    }
   ],
   "source": [
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import optuna\n",
    "\n",
    "# For data preprocess\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "# 设置随机种子\n",
    "myseed = 666\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(myseed)\n",
    "torch.manual_seed(myseed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(myseed)\n",
    "\n",
    "# 获取设备\n",
    "def get_device():\n",
    "    return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "# 数据处理类\n",
    "class Dataprocess(Dataset):\n",
    "    def __init__(self, path, mode='train', modify=True):\n",
    "        self.mode = mode\n",
    "        with open(path, 'r') as f:\n",
    "            data = list(csv.reader(f))\n",
    "            data = np.array(data[1:])[:, 1:].astype(float)\n",
    "        if modify == False:\n",
    "            feats = list(range(0, 93))\n",
    "        else:\n",
    "            day1_feats = list(range(40, 58))\n",
    "            day2_feats = list(range(58, 76))\n",
    "            day3_feats = list(range(76, 93))\n",
    "            feats = day1_feats + day2_feats + day3_feats\n",
    "        if mode == 'test':\n",
    "            feats = [f + 1 for f in feats]\n",
    "            data = data[:, feats]\n",
    "            self.data = torch.FloatTensor(data)\n",
    "        else:\n",
    "            target = data[:, -1]\n",
    "            data = data[:, feats]\n",
    "            if mode == 'train':\n",
    "                indices = [i for i in range(len(data)) if i % 10 != 0]\n",
    "            elif mode == 'dev':\n",
    "                indices = [i for i in range(len(data)) if i % 10 == 0]\n",
    "            self.data = torch.FloatTensor(data[indices])\n",
    "            self.target = torch.FloatTensor(target[indices])\n",
    "        self.data[:, 40:] = (self.data[:, 40:] - self.data[:, 40:].mean(dim=0, keepdim=True)) / self.data[:, 40:].std(dim=0, keepdim=True)\n",
    "        self.dim = self.data.shape[1]\n",
    "        print('Finished reading the {} set of COVID19 Dataset ({} samples found, each dim = {})'.format(mode, len(self.data), self.dim))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.mode in ['train', 'dev']:\n",
    "            return self.data[index], self.target[index]\n",
    "        else:\n",
    "            return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "# 数据加载器\n",
    "def dataloader(path, mode, batch_size, n_jobs=0, modify=False):\n",
    "    dataset = Dataprocess(path, mode=mode, modify=modify)\n",
    "    dataloader = DataLoader(dataset, batch_size, shuffle=(mode == 'train'), drop_last=False, num_workers=n_jobs, pin_memory=True)\n",
    "    return dataloader\n",
    "\n",
    "# 神经网络模型\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        tryit = 2048\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, tryit),\n",
    "            nn.BatchNorm1d(tryit),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.Linear(tryit, 1)\n",
    "        )\n",
    "        self.criterion = nn.SmoothL1Loss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(1)\n",
    "\n",
    "    def cal_loss(self, pred, target):\n",
    "        return self.criterion(pred, target)\n",
    "\n",
    "# 训练函数\n",
    "def train(tr_set, dv_set, model, config, device):\n",
    "    n_epochs = config['n_epochs']\n",
    "    optimizer = getattr(torch.optim, config['optimizer'])(model.parameters(), **config['optim_hparas'])\n",
    "    min_mse = 1000.\n",
    "    loss_record = {'train': [], 'dev': []}\n",
    "    early_stop_cnt = 0\n",
    "    epoch = 0\n",
    "    while epoch < n_epochs:\n",
    "        model.train()\n",
    "        for x, y in tr_set:\n",
    "            optimizer.zero_grad()\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pred = model(x)\n",
    "            mse_loss = model.cal_loss(pred, y)\n",
    "            mse_loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_record['train'].append(mse_loss.detach().cpu().item())\n",
    "        dev_mse = dev(dv_set, model, device)\n",
    "        if dev_mse < min_mse:\n",
    "            min_mse = dev_mse\n",
    "            print('Saving model (epoch = {:4d}, loss = {:.4f})'.format(epoch + 1, min_mse))\n",
    "            torch.save(model.state_dict(), config['save_path'])\n",
    "            early_stop_cnt = 0\n",
    "        else:\n",
    "            early_stop_cnt += 1\n",
    "        epoch += 1\n",
    "        loss_record['dev'].append(dev_mse)\n",
    "        if early_stop_cnt > config['early_stop']:\n",
    "            break\n",
    "    print('Finished training after {} epochs'.format(epoch))\n",
    "    return min_mse, loss_record\n",
    "\n",
    "# 验证函数\n",
    "def dev(dv_set, model, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for x, y in dv_set:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = model(x)\n",
    "            mse_loss = model.cal_loss(pred, y)\n",
    "        total_loss += mse_loss.detach().cpu().item() * len(x)\n",
    "    total_loss = total_loss / len(dv_set.dataset)\n",
    "    return total_loss\n",
    "\n",
    "# 测试函数\n",
    "def test(tt_set, model, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    for x in tt_set:\n",
    "        x = x.to(torch.device('cpu'))\n",
    "        with torch.no_grad():\n",
    "            pred = model(x)\n",
    "            preds.append(pred.detach().cpu())\n",
    "    preds = torch.cat(preds, dim=0).numpy()\n",
    "    return preds\n",
    "\n",
    "\n",
    "# 主程序\n",
    "device = torch.device('cpu')\n",
    "os.makedirs('models', exist_ok=True)\n",
    "modify = True\n",
    "config = {\n",
    "    'n_epochs': 5000,                \n",
    "    'batch_size': 256,               \n",
    "    'optimizer': 'RMSprop',              \n",
    "    'optim_hparas': {                \n",
    "        'lr': 0.0005,\n",
    "        'alpha':0.99, \n",
    "        'eps':1e-8,\n",
    "        'weight_decay': 0.001,         \n",
    "    },\n",
    "    'early_stop': 500,               \n",
    "    'save_path': 'models/model.pth'  \n",
    "}\n",
    "\n",
    "\n",
    "train_path = '/Users/johnny/Downloads/24-fall-sdsc-8007-hw-1/HW1.train.csv'  # path to training data\n",
    "test_path = '/Users/johnny/Downloads/24-fall-sdsc-8007-hw-1/HW1.test.csv'   # path to testing data\n",
    "\n",
    "train_set = dataloader(train_path, 'train', config['batch_size'], modify=modify)\n",
    "validation_set = dataloader(train_path, 'dev', config['batch_size'], modify=modify)\n",
    "test_set = dataloader(test_path, 'test', config['batch_size'], modify=modify)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # 定义超参数搜索空间\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-1, log=True)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 256, log=True)\n",
    "    n_epochs = trial.suggest_int('n_epochs', 100, 5000)\n",
    "    \n",
    "    # 定义模型\n",
    "    model = NeuralNet(train_set.dataset.dim).to(torch.device('cpu'))\n",
    "    \n",
    "    # 定义优化器\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "    \n",
    "    # 定义数据加载器\n",
    "    train_loader =  dataloader(train_path, 'train', config['batch_size'], modify=modify)\n",
    "    val_loader = dataloader(train_path, 'dev', config['batch_size'], modify=modify)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to('cpu'), y.to('cpu')\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(x)\n",
    "            loss = model.cal_loss(pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # 验证模型\n",
    "    val_loss = dev(val_loader, model, device)\n",
    "    \n",
    "    return val_loss\n",
    "# 创建Optuna的Study对象\n",
    "study = optuna.create_study(direction='minimize')\n",
    "# 运行优化\n",
    "study.optimize(objective, n_trials=1000)\n",
    "\n",
    "# 输出最佳超参数\n",
    "print('Best hyperparameters: ', study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8df8b6c-b6ec-45ef-b812-e71be0df85d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-26 12:45:54,281] A new study created in memory with name: no-name-9b31783e-6a82-4969-969b-afc944c7d8b7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading the train set of COVID19 Dataset (1800 samples found, each dim = 53)\n",
      "Finished reading the dev set of COVID19 Dataset (200 samples found, each dim = 53)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-26 12:46:21,331] Trial 0 finished with value: 0.619589250087738 and parameters: {'lr': 0.00062222509499192, 'batch_size': 126, 'n_epochs': 1339, 'n_layers': 1, 'n_neurons': 644, 'optimizer': 'Adam'}. Best is trial 0 with value: 0.619589250087738.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading the train set of COVID19 Dataset (1800 samples found, each dim = 53)\n",
      "Finished reading the dev set of COVID19 Dataset (200 samples found, each dim = 53)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-26 12:46:45,826] Trial 1 finished with value: 1.7793535149097444 and parameters: {'lr': 0.06777660655172024, 'batch_size': 166, 'n_epochs': 1242, 'n_layers': 1, 'n_neurons': 675, 'optimizer': 'AdamW'}. Best is trial 0 with value: 0.619589250087738.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading the train set of COVID19 Dataset (1800 samples found, each dim = 53)\n",
      "Finished reading the dev set of COVID19 Dataset (200 samples found, each dim = 53)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-26 12:47:11,608] Trial 2 finished with value: 0.7193797826766968 and parameters: {'lr': 0.015322682048718969, 'batch_size': 104, 'n_epochs': 1066, 'n_layers': 1, 'n_neurons': 671, 'optimizer': 'AdamW'}. Best is trial 0 with value: 0.619589250087738.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading the train set of COVID19 Dataset (1800 samples found, each dim = 53)\n",
      "Finished reading the dev set of COVID19 Dataset (200 samples found, each dim = 53)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-26 12:47:48,784] Trial 3 finished with value: 1.8955415219068528 and parameters: {'lr': 0.04340500500976208, 'batch_size': 115, 'n_epochs': 1260, 'n_layers': 1, 'n_neurons': 1030, 'optimizer': 'AdamW'}. Best is trial 0 with value: 0.619589250087738.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading the train set of COVID19 Dataset (1800 samples found, each dim = 53)\n",
      "Finished reading the dev set of COVID19 Dataset (200 samples found, each dim = 53)\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "myseed = 666\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(myseed)\n",
    "torch.manual_seed(myseed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(myseed)\n",
    "\n",
    "# Get device\n",
    "def get_device():\n",
    "    return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Data processing class\n",
    "class Dataprocess(Dataset):\n",
    "    def __init__(self, path, mode='train', modify=True):\n",
    "        self.mode = mode\n",
    "        with open(path, 'r') as f:\n",
    "            data = list(csv.reader(f))\n",
    "            data = np.array(data[1:])[:, 1:].astype(float)\n",
    "        if modify == False:\n",
    "            feats = list(range(0, 93))\n",
    "        else:\n",
    "            day1_feats = list(range(40, 58))\n",
    "            day2_feats = list(range(58, 76))\n",
    "            day3_feats = list(range(76, 93))\n",
    "            feats = day1_feats + day2_feats + day3_feats\n",
    "        if mode == 'test':\n",
    "            feats = [f + 1 for f in feats]\n",
    "            data = data[:, feats]\n",
    "            self.data = torch.FloatTensor(data)\n",
    "        else:\n",
    "            target = data[:, -1]\n",
    "            data = data[:, feats]\n",
    "            if mode == 'train':\n",
    "                indices = [i for i in range(len(data)) if i % 10 != 0]\n",
    "            elif mode == 'dev':\n",
    "                indices = [i for i in range(len(data)) if i % 10 == 0]\n",
    "            self.data = torch.FloatTensor(data[indices])\n",
    "            self.target = torch.FloatTensor(target[indices])\n",
    "        self.data[:, 40:] = (self.data[:, 40:] - self.data[:, 40:].mean(dim=0, keepdim=True)) / self.data[:, 40:].std(dim=0, keepdim=True)\n",
    "        self.dim = self.data.shape[1]\n",
    "        print('Finished reading the {} set of COVID19 Dataset ({} samples found, each dim = {})'.format(mode, len(self.data), self.dim))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.mode in ['train', 'dev']:\n",
    "            return self.data[index], self.target[index]\n",
    "        else:\n",
    "            return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "# Data loader\n",
    "def dataloader(path, mode, batch_size, n_jobs=0, modify=False):\n",
    "    dataset = Dataprocess(path, mode=mode, modify=modify)\n",
    "    dataloader = DataLoader(dataset, batch_size, shuffle=(mode == 'train'), drop_last=False, num_workers=n_jobs, pin_memory=True)\n",
    "    return dataloader\n",
    "\n",
    "# Neural network model\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_dim, n_layers, n_neurons):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        layers = []\n",
    "        for _ in range(n_layers):\n",
    "            layers.append(nn.Linear(input_dim, n_neurons))\n",
    "            layers.append(nn.BatchNorm1d(n_neurons))\n",
    "            layers.append(nn.LeakyReLU(negative_slope=0.01))\n",
    "            input_dim = n_neurons\n",
    "        layers.append(nn.Linear(n_neurons, 1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.criterion = nn.SmoothL1Loss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(1)\n",
    "\n",
    "    def cal_loss(self, pred, target):\n",
    "        return self.criterion(pred, target)\n",
    "\n",
    "# Training function\n",
    "def train(tr_set, dv_set, model, config, device):\n",
    "    n_epochs = config['n_epochs']\n",
    "    optimizer = getattr(torch.optim, config['optimizer'])(model.parameters(), **config['optim_hparas'])\n",
    "    min_mse = 1000.\n",
    "    loss_record = {'train': [], 'dev': []}\n",
    "    early_stop_cnt = 0\n",
    "    epoch = 0\n",
    "    while epoch < n_epochs:\n",
    "        model.train()\n",
    "        for x, y in tr_set:\n",
    "            optimizer.zero_grad()\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pred = model(x)\n",
    "            mse_loss = model.cal_loss(pred, y)\n",
    "            mse_loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_record['train'].append(mse_loss.detach().cpu().item())\n",
    "        dev_mse = dev(dv_set, model, device)\n",
    "        if dev_mse < min_mse:\n",
    "            min_mse = dev_mse\n",
    "            print('Saving model (epoch = {:4d}, loss = {:.4f})'.format(epoch + 1, min_mse))\n",
    "            torch.save(model.state_dict(), config['save_path'])\n",
    "            early_stop_cnt = 0\n",
    "        else:\n",
    "            early_stop_cnt += 1\n",
    "        epoch += 1\n",
    "        loss_record['dev'].append(dev_mse)\n",
    "        if early_stop_cnt > config['early_stop']:\n",
    "            break\n",
    "    print('Finished training after {} epochs'.format(epoch))\n",
    "    return min_mse, loss_record\n",
    "\n",
    "# Validation function\n",
    "def dev(dv_set, model, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for x, y in dv_set:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = model(x)\n",
    "            mse_loss = model.cal_loss(pred, y)\n",
    "        total_loss += mse_loss.detach().cpu().item() * len(x)\n",
    "    total_loss = total_loss / len(dv_set.dataset)\n",
    "    return total_loss\n",
    "\n",
    "# Testing function\n",
    "def test(tt_set, model, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    for x in tt_set:\n",
    "        x = x.to(torch.device('cpu'))\n",
    "        with torch.no_grad():\n",
    "            pred = model(x)\n",
    "            preds.append(pred.detach().cpu())\n",
    "    preds = torch.cat(preds, dim=0).numpy()\n",
    "    return preds\n",
    "\n",
    "# Main program\n",
    "device = torch.device('cpu')\n",
    "os.makedirs('models', exist_ok=True)\n",
    "modify = True\n",
    "\n",
    "\n",
    "train_path = '/Users/johnny/Downloads/24-fall-sdsc-8007-hw-1/HW1.train.csv'  # path to training data\n",
    "test_path = '/Users/johnny/Downloads/24-fall-sdsc-8007-hw-1/HW1.test.csv'   # path to testing data\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameter search space\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-1, log=True)\n",
    "    batch_size = trial.suggest_int('batch_size', 100, 200, log=True)\n",
    "    n_epochs = trial.suggest_int('n_epochs', 1000, 1500)\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 1)\n",
    "    n_neurons = trial.suggest_int('n_neurons', 500, 1500, log=True)\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'AdamW'])\n",
    "\n",
    "    # Define data loaders\n",
    "    train_set = dataloader(train_path, 'train', batch_size, modify=modify)\n",
    "    val_set = dataloader(train_path, 'dev', batch_size, modify=modify)\n",
    "    \n",
    "    # Define model\n",
    "    model = NeuralNet(train_set.dataset.dim, n_layers, n_neurons).to(device)\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer = getattr(torch.optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "\n",
    "    \n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        for x, y in train_set:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(x)\n",
    "            loss = model.cal_loss(pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Validate model\n",
    "    val_loss = dev(val_set, model, device)\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "# Create Optuna study\n",
    "study = optuna.create_study(direction='minimize')\n",
    "# Run optimization\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Output best hyperparameters\n",
    "print('Best hyperparameters: ', study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a410361-6802-4f13-970a-da672c151140",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "8007",
   "language": "python",
   "name": "8007"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
