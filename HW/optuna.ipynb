{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8df8b6c-b6ec-45ef-b812-e71be0df85d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-29 22:01:12,439] A new study created in memory with name: no-name-0884a712-6812-4959-9107-f087e5ff624d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda Training\n",
      "Finished reading the train set of COVID19 Dataset (1800 samples found, each dim = 14)\n",
      "Finished reading the dev set of COVID19 Dataset (200 samples found, each dim = 14)\n",
      "Epoch 00500: reducing learning rate of group 0 to 1.3886e-04.\n",
      "Epoch 00564: reducing learning rate of group 0 to 6.9429e-05.\n",
      "Epoch 00615: reducing learning rate of group 0 to 3.4715e-05.\n",
      "Epoch 00666: reducing learning rate of group 0 to 1.7357e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-29 22:01:57,051] Trial 0 finished with value: 0.7691909605264664 and parameters: {'lr': 0.0002777170652407728, 'batch_size': 36, 'n_epochs': 5282, 'early_stop': 165, 'n_neurons': 101, 'optimizer': 'Adam'}. Best is trial 0 with value: 0.7691909605264664.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training after 679 epochs\n",
      "Finished reading the train set of COVID19 Dataset (1800 samples found, each dim = 14)\n",
      "Finished reading the dev set of COVID19 Dataset (200 samples found, each dim = 14)\n",
      "Epoch 00876: reducing learning rate of group 0 to 1.8968e-04.\n",
      "Epoch 00977: reducing learning rate of group 0 to 9.4840e-05.\n",
      "Epoch 01028: reducing learning rate of group 0 to 4.7420e-05.\n",
      "Epoch 01100: reducing learning rate of group 0 to 2.3710e-05.\n",
      "Epoch 01184: reducing learning rate of group 0 to 1.1855e-05.\n",
      "Epoch 01235: reducing learning rate of group 0 to 5.9275e-06.\n",
      "Epoch 01286: reducing learning rate of group 0 to 2.9638e-06.\n",
      "Epoch 01337: reducing learning rate of group 0 to 1.4819e-06.\n",
      "Epoch 01388: reducing learning rate of group 0 to 7.4094e-07.\n",
      "Epoch 01439: reducing learning rate of group 0 to 3.7047e-07.\n",
      "Epoch 01490: reducing learning rate of group 0 to 1.8523e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-29 22:02:33,510] Trial 1 finished with value: 0.7453734457492829 and parameters: {'lr': 0.00037936050697327085, 'batch_size': 140, 'n_epochs': 2022, 'early_stop': 402, 'n_neurons': 72, 'optimizer': 'Adam'}. Best is trial 1 with value: 0.7453734457492829.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training after 1536 epochs\n",
      "Finished reading the train set of COVID19 Dataset (1800 samples found, each dim = 14)\n",
      "Finished reading the dev set of COVID19 Dataset (200 samples found, each dim = 14)\n",
      "Epoch 02285: reducing learning rate of group 0 to 3.3000e-05.\n",
      "Epoch 02390: reducing learning rate of group 0 to 1.6500e-05.\n",
      "Epoch 02441: reducing learning rate of group 0 to 8.2500e-06.\n",
      "Epoch 02497: reducing learning rate of group 0 to 4.1250e-06.\n",
      "Epoch 02548: reducing learning rate of group 0 to 2.0625e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-29 22:04:05,813] Trial 2 finished with value: 0.8252995014190674 and parameters: {'lr': 6.599998815162135e-05, 'batch_size': 100, 'n_epochs': 2629, 'early_stop': 137, 'n_neurons': 207, 'optimizer': 'Adam'}. Best is trial 1 with value: 0.7453734457492829.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training after 2584 epochs\n",
      "Finished reading the train set of COVID19 Dataset (1800 samples found, each dim = 14)\n",
      "Finished reading the dev set of COVID19 Dataset (200 samples found, each dim = 14)\n",
      "Epoch 00404: reducing learning rate of group 0 to 2.4920e-04.\n",
      "Epoch 00489: reducing learning rate of group 0 to 1.2460e-04.\n",
      "Epoch 00540: reducing learning rate of group 0 to 6.2300e-05.\n",
      "Epoch 00591: reducing learning rate of group 0 to 3.1150e-05.\n",
      "Epoch 00642: reducing learning rate of group 0 to 1.5575e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-29 22:04:33,269] Trial 3 finished with value: 0.749091693609953 and parameters: {'lr': 0.0004983985866085923, 'batch_size': 63, 'n_epochs': 4382, 'early_stop': 237, 'n_neurons': 88, 'optimizer': 'Adam'}. Best is trial 1 with value: 0.7453734457492829.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training after 676 epochs\n",
      "Finished reading the train set of COVID19 Dataset (1800 samples found, each dim = 14)\n",
      "Finished reading the dev set of COVID19 Dataset (200 samples found, each dim = 14)\n",
      "Epoch 01480: reducing learning rate of group 0 to 6.2485e-05.\n",
      "Epoch 01826: reducing learning rate of group 0 to 3.1243e-05.\n",
      "Epoch 01877: reducing learning rate of group 0 to 1.5621e-05.\n",
      "Epoch 01954: reducing learning rate of group 0 to 7.8107e-06.\n",
      "Epoch 02005: reducing learning rate of group 0 to 3.9053e-06.\n",
      "Epoch 02056: reducing learning rate of group 0 to 1.9527e-06.\n",
      "Epoch 02161: reducing learning rate of group 0 to 9.7633e-07.\n",
      "Epoch 02212: reducing learning rate of group 0 to 4.8817e-07.\n",
      "Epoch 02263: reducing learning rate of group 0 to 2.4408e-07.\n",
      "Epoch 02314: reducing learning rate of group 0 to 1.2204e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-29 22:05:46,209] Trial 4 finished with value: 0.7911521908640862 and parameters: {'lr': 0.00012497046291252316, 'batch_size': 123, 'n_epochs': 3218, 'early_stop': 223, 'n_neurons': 201, 'optimizer': 'Adam'}. Best is trial 1 with value: 0.7453734457492829.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training after 2334 epochs\n",
      "Finished reading the train set of COVID19 Dataset (1800 samples found, each dim = 14)\n",
      "Finished reading the dev set of COVID19 Dataset (200 samples found, each dim = 14)\n",
      "Epoch 00638: reducing learning rate of group 0 to 2.4207e-04.\n",
      "Epoch 00772: reducing learning rate of group 0 to 1.2103e-04.\n",
      "Epoch 00823: reducing learning rate of group 0 to 6.0517e-05.\n",
      "Epoch 00874: reducing learning rate of group 0 to 3.0258e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-29 22:06:11,990] Trial 5 finished with value: 0.7730578270554542 and parameters: {'lr': 0.00048413457923275274, 'batch_size': 137, 'n_epochs': 5131, 'early_stop': 196, 'n_neurons': 170, 'optimizer': 'Adam'}. Best is trial 1 with value: 0.7453734457492829.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training after 918 epochs\n",
      "Finished reading the train set of COVID19 Dataset (1800 samples found, each dim = 14)\n",
      "Finished reading the dev set of COVID19 Dataset (200 samples found, each dim = 14)\n",
      "Epoch 00494: reducing learning rate of group 0 to 1.0606e-05.\n",
      "Epoch 00545: reducing learning rate of group 0 to 5.3032e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-29 22:06:25,372] Trial 6 finished with value: 23.511608123779297 and parameters: {'lr': 2.1212914820679353e-05, 'batch_size': 232, 'n_epochs': 1645, 'early_stop': 125, 'n_neurons': 298, 'optimizer': 'SGD'}. Best is trial 1 with value: 0.7453734457492829.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training after 569 epochs\n",
      "Finished reading the train set of COVID19 Dataset (1800 samples found, each dim = 14)\n",
      "Finished reading the dev set of COVID19 Dataset (200 samples found, each dim = 14)\n",
      "Epoch 00693: reducing learning rate of group 0 to 9.0012e-05.\n",
      "Epoch 00837: reducing learning rate of group 0 to 4.5006e-05.\n",
      "Epoch 00929: reducing learning rate of group 0 to 2.2503e-05.\n",
      "Epoch 00980: reducing learning rate of group 0 to 1.1251e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-29 22:07:14,472] Trial 7 finished with value: 0.7530630081892014 and parameters: {'lr': 0.00018002363729685157, 'batch_size': 50, 'n_epochs': 2171, 'early_stop': 130, 'n_neurons': 76, 'optimizer': 'Adam'}. Best is trial 1 with value: 0.7453734457492829.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training after 1009 epochs\n",
      "Finished reading the train set of COVID19 Dataset (1800 samples found, each dim = 14)\n",
      "Finished reading the dev set of COVID19 Dataset (200 samples found, each dim = 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-29 22:07:49,753] Trial 8 finished with value: 7.251877212524414 and parameters: {'lr': 2.954165397229522e-05, 'batch_size': 102, 'n_epochs': 1231, 'early_stop': 213, 'n_neurons': 54, 'optimizer': 'Adam'}. Best is trial 1 with value: 0.7453734457492829.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training after 1231 epochs\n",
      "Finished reading the train set of COVID19 Dataset (1800 samples found, each dim = 14)\n",
      "Finished reading the dev set of COVID19 Dataset (200 samples found, each dim = 14)\n",
      "Epoch 00927: reducing learning rate of group 0 to 6.2105e-06.\n",
      "Epoch 01018: reducing learning rate of group 0 to 3.1052e-06.\n",
      "Epoch 01069: reducing learning rate of group 0 to 1.5526e-06.\n",
      "Epoch 01120: reducing learning rate of group 0 to 7.7631e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-29 22:08:20,719] Trial 9 finished with value: 8.703920395374299 and parameters: {'lr': 1.2420990699058878e-05, 'batch_size': 97, 'n_epochs': 1220, 'early_stop': 171, 'n_neurons': 52, 'optimizer': 'SGD'}. Best is trial 1 with value: 0.7453734457492829.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training after 1139 epochs\n",
      "Finished reading the train set of COVID19 Dataset (1800 samples found, each dim = 14)\n",
      "Finished reading the dev set of COVID19 Dataset (200 samples found, each dim = 14)\n",
      "Epoch 00746: reducing learning rate of group 0 to 4.8320e-04.\n",
      "Epoch 00824: reducing learning rate of group 0 to 2.4160e-04.\n",
      "Epoch 00955: reducing learning rate of group 0 to 1.2080e-04.\n",
      "Epoch 01023: reducing learning rate of group 0 to 6.0400e-05.\n",
      "Epoch 01074: reducing learning rate of group 0 to 3.0200e-05.\n",
      "Epoch 01125: reducing learning rate of group 0 to 1.5100e-05.\n",
      "Epoch 01176: reducing learning rate of group 0 to 7.5500e-06.\n",
      "Epoch 01227: reducing learning rate of group 0 to 3.7750e-06.\n",
      "Epoch 01278: reducing learning rate of group 0 to 1.8875e-06.\n",
      "Epoch 01329: reducing learning rate of group 0 to 9.4375e-07.\n",
      "Epoch 01380: reducing learning rate of group 0 to 4.7187e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-29 22:08:50,749] Trial 10 finished with value: 0.8824907541275024 and parameters: {'lr': 0.0009663983261052768, 'batch_size': 207, 'n_epochs': 1876, 'early_stop': 456, 'n_neurons': 134, 'optimizer': 'SGD'}. Best is trial 1 with value: 0.7453734457492829.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training after 1429 epochs\n",
      "Finished reading the train set of COVID19 Dataset (1800 samples found, each dim = 14)\n",
      "Finished reading the dev set of COVID19 Dataset (200 samples found, each dim = 14)\n",
      "Epoch 00343: reducing learning rate of group 0 to 2.8365e-04.\n",
      "Epoch 00461: reducing learning rate of group 0 to 1.4183e-04.\n",
      "Epoch 00512: reducing learning rate of group 0 to 7.0913e-05.\n",
      "Epoch 00563: reducing learning rate of group 0 to 3.5456e-05.\n",
      "Epoch 00614: reducing learning rate of group 0 to 1.7728e-05.\n",
      "Epoch 00665: reducing learning rate of group 0 to 8.8641e-06.\n",
      "Epoch 00716: reducing learning rate of group 0 to 4.4321e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-29 22:09:22,768] Trial 11 finished with value: 0.7402272905409336 and parameters: {'lr': 0.0005673032930370957, 'batch_size': 59, 'n_epochs': 3538, 'early_stop': 336, 'n_neurons': 79, 'optimizer': 'Adam'}. Best is trial 11 with value: 0.7402272905409336.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training after 747 epochs\n",
      "Finished reading the train set of COVID19 Dataset (1800 samples found, each dim = 14)\n",
      "Finished reading the dev set of COVID19 Dataset (200 samples found, each dim = 14)\n",
      "Epoch 00298: reducing learning rate of group 0 to 4.5924e-04.\n",
      "Epoch 00349: reducing learning rate of group 0 to 2.2962e-04.\n",
      "Epoch 00400: reducing learning rate of group 0 to 1.1481e-04.\n",
      "Epoch 00451: reducing learning rate of group 0 to 5.7405e-05.\n",
      "Epoch 00516: reducing learning rate of group 0 to 2.8702e-05.\n",
      "Epoch 00590: reducing learning rate of group 0 to 1.4351e-05.\n",
      "Epoch 00644: reducing learning rate of group 0 to 7.1756e-06.\n",
      "Epoch 00695: reducing learning rate of group 0 to 3.5878e-06.\n",
      "Epoch 00746: reducing learning rate of group 0 to 1.7939e-06.\n",
      "Epoch 00797: reducing learning rate of group 0 to 8.9695e-07.\n",
      "Epoch 00848: reducing learning rate of group 0 to 4.4848e-07.\n",
      "Epoch 00959: reducing learning rate of group 0 to 2.2424e-07.\n",
      "Epoch 01010: reducing learning rate of group 0 to 1.1212e-07.\n",
      "Epoch 01061: reducing learning rate of group 0 to 5.6059e-08.\n",
      "Epoch 01112: reducing learning rate of group 0 to 2.8030e-08.\n",
      "Epoch 01163: reducing learning rate of group 0 to 1.4015e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-29 22:10:13,771] Trial 12 finished with value: 0.7602245399355888 and parameters: {'lr': 0.0009184783586166136, 'batch_size': 63, 'n_epochs': 3362, 'early_stop': 387, 'n_neurons': 72, 'optimizer': 'Adam'}. Best is trial 11 with value: 0.7402272905409336.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training after 1296 epochs\n",
      "Finished reading the train set of COVID19 Dataset (1800 samples found, each dim = 14)\n",
      "Finished reading the dev set of COVID19 Dataset (200 samples found, each dim = 14)\n",
      "Epoch 01003: reducing learning rate of group 0 to 1.5768e-04.\n",
      "Epoch 01064: reducing learning rate of group 0 to 7.8838e-05.\n",
      "Epoch 01142: reducing learning rate of group 0 to 3.9419e-05.\n",
      "Epoch 01193: reducing learning rate of group 0 to 1.9710e-05.\n",
      "Epoch 01244: reducing learning rate of group 0 to 9.8548e-06.\n",
      "Epoch 01295: reducing learning rate of group 0 to 4.9274e-06.\n",
      "Epoch 01395: reducing learning rate of group 0 to 2.4637e-06.\n",
      "Epoch 01479: reducing learning rate of group 0 to 1.2318e-06.\n",
      "Epoch 01530: reducing learning rate of group 0 to 6.1592e-07.\n",
      "Epoch 01614: reducing learning rate of group 0 to 3.0796e-07.\n",
      "Epoch 01665: reducing learning rate of group 0 to 1.5398e-07.\n",
      "Epoch 01716: reducing learning rate of group 0 to 7.6990e-08.\n",
      "Epoch 01767: reducing learning rate of group 0 to 3.8495e-08.\n",
      "Epoch 01818: reducing learning rate of group 0 to 1.9248e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-29 22:10:56,684] Trial 13 finished with value: 0.7444217032194138 and parameters: {'lr': 0.00031535278536825255, 'batch_size': 158, 'n_epochs': 3077, 'early_stop': 317, 'n_neurons': 67, 'optimizer': 'Adam'}. Best is trial 11 with value: 0.7402272905409336.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training after 1881 epochs\n",
      "Finished reading the train set of COVID19 Dataset (1800 samples found, each dim = 14)\n",
      "Finished reading the dev set of COVID19 Dataset (200 samples found, each dim = 14)\n",
      "Epoch 00387: reducing learning rate of group 0 to 1.0301e-04.\n",
      "Epoch 00508: reducing learning rate of group 0 to 5.1504e-05.\n",
      "Epoch 00607: reducing learning rate of group 0 to 2.5752e-05.\n",
      "Epoch 00658: reducing learning rate of group 0 to 1.2876e-05.\n",
      "Epoch 00720: reducing learning rate of group 0 to 6.4379e-06.\n",
      "Epoch 00771: reducing learning rate of group 0 to 3.2190e-06.\n",
      "Epoch 00822: reducing learning rate of group 0 to 1.6095e-06.\n",
      "Epoch 00873: reducing learning rate of group 0 to 8.0474e-07.\n",
      "Epoch 00924: reducing learning rate of group 0 to 4.0237e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-29 22:11:31,684] Trial 14 finished with value: 1.2127220159769059 and parameters: {'lr': 0.00020601430031901076, 'batch_size': 69, 'n_epochs': 3486, 'early_stop': 299, 'n_neurons': 117, 'optimizer': 'SGD'}. Best is trial 11 with value: 0.7402272905409336.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training after 969 epochs\n",
      "Finished reading the train set of COVID19 Dataset (1800 samples found, each dim = 14)\n",
      "Finished reading the dev set of COVID19 Dataset (200 samples found, each dim = 14)\n",
      "Epoch 00989: reducing learning rate of group 0 to 3.4845e-05.\n",
      "Epoch 01182: reducing learning rate of group 0 to 1.7422e-05.\n",
      "Epoch 01284: reducing learning rate of group 0 to 8.7112e-06.\n",
      "Epoch 01335: reducing learning rate of group 0 to 4.3556e-06.\n",
      "Epoch 01386: reducing learning rate of group 0 to 2.1778e-06.\n",
      "Epoch 01461: reducing learning rate of group 0 to 1.0889e-06.\n",
      "Epoch 01512: reducing learning rate of group 0 to 5.4445e-07.\n",
      "Epoch 01563: reducing learning rate of group 0 to 2.7222e-07.\n",
      "Epoch 01614: reducing learning rate of group 0 to 1.3611e-07.\n",
      "Epoch 01665: reducing learning rate of group 0 to 6.8056e-08.\n",
      "Epoch 01723: reducing learning rate of group 0 to 3.4028e-08.\n",
      "Epoch 01774: reducing learning rate of group 0 to 1.7014e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-29 22:14:03,235] Trial 15 finished with value: 0.8302787590026856 and parameters: {'lr': 6.968942856323152e-05, 'batch_size': 32, 'n_epochs': 2601, 'early_stop': 308, 'n_neurons': 64, 'optimizer': 'Adam'}. Best is trial 11 with value: 0.7402272905409336.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training after 1981 epochs\n",
      "Finished reading the train set of COVID19 Dataset (1800 samples found, each dim = 14)\n",
      "Finished reading the dev set of COVID19 Dataset (200 samples found, each dim = 14)\n",
      "Epoch 00702: reducing learning rate of group 0 to 3.0622e-04.\n",
      "Epoch 00906: reducing learning rate of group 0 to 1.5311e-04.\n",
      "Epoch 00986: reducing learning rate of group 0 to 7.6555e-05.\n",
      "Epoch 01037: reducing learning rate of group 0 to 3.8278e-05.\n",
      "Epoch 01113: reducing learning rate of group 0 to 1.9139e-05.\n",
      "Epoch 01164: reducing learning rate of group 0 to 9.5694e-06.\n",
      "Epoch 01215: reducing learning rate of group 0 to 4.7847e-06.\n",
      "Epoch 01266: reducing learning rate of group 0 to 2.3924e-06.\n",
      "Epoch 01317: reducing learning rate of group 0 to 1.1962e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-29 22:14:33,826] Trial 16 finished with value: 0.7581505715847016 and parameters: {'lr': 0.0006124417256686076, 'batch_size': 180, 'n_epochs': 4130, 'early_stop': 295, 'n_neurons': 94, 'optimizer': 'Adam'}. Best is trial 11 with value: 0.7402272905409336.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training after 1358 epochs\n",
      "Finished reading the train set of COVID19 Dataset (1800 samples found, each dim = 14)\n",
      "Finished reading the dev set of COVID19 Dataset (200 samples found, each dim = 14)\n",
      "Epoch 00514: reducing learning rate of group 0 to 1.3915e-04.\n",
      "Epoch 00565: reducing learning rate of group 0 to 6.9577e-05.\n",
      "Epoch 00690: reducing learning rate of group 0 to 3.4788e-05.\n",
      "Epoch 00741: reducing learning rate of group 0 to 1.7394e-05.\n",
      "Epoch 00792: reducing learning rate of group 0 to 8.6971e-06.\n",
      "Epoch 00843: reducing learning rate of group 0 to 4.3485e-06.\n",
      "Epoch 00894: reducing learning rate of group 0 to 2.1743e-06.\n",
      "Epoch 00945: reducing learning rate of group 0 to 1.0871e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-29 22:15:32,709] Trial 17 finished with value: 0.7633347591757774 and parameters: {'lr': 0.0002783063224911175, 'batch_size': 43, 'n_epochs': 2943, 'early_stop': 350, 'n_neurons': 61, 'optimizer': 'Adam'}. Best is trial 11 with value: 0.7402272905409336.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training after 990 epochs\n",
      "Finished reading the train set of COVID19 Dataset (1800 samples found, each dim = 14)\n",
      "Finished reading the dev set of COVID19 Dataset (200 samples found, each dim = 14)\n",
      "Epoch 00434: reducing learning rate of group 0 to 5.8521e-05.\n",
      "Epoch 00792: reducing learning rate of group 0 to 2.9261e-05.\n",
      "Epoch 00878: reducing learning rate of group 0 to 1.4630e-05.\n",
      "Epoch 00945: reducing learning rate of group 0 to 7.3152e-06.\n",
      "Epoch 01038: reducing learning rate of group 0 to 3.6576e-06.\n",
      "Epoch 01089: reducing learning rate of group 0 to 1.8288e-06.\n",
      "Epoch 01140: reducing learning rate of group 0 to 9.1440e-07.\n",
      "Epoch 01191: reducing learning rate of group 0 to 4.5720e-07.\n",
      "Epoch 01242: reducing learning rate of group 0 to 2.2860e-07.\n",
      "Epoch 01293: reducing learning rate of group 0 to 1.1430e-07.\n",
      "Epoch 01344: reducing learning rate of group 0 to 5.7150e-08.\n",
      "Epoch 01395: reducing learning rate of group 0 to 2.8575e-08.\n",
      "Epoch 01446: reducing learning rate of group 0 to 1.4287e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-29 22:16:25,969] Trial 18 finished with value: 2.908344702720642 and parameters: {'lr': 0.00011704277527251544, 'batch_size': 76, 'n_epochs': 3995, 'early_stop': 488, 'n_neurons': 128, 'optimizer': 'SGD'}. Best is trial 11 with value: 0.7402272905409336.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training after 1476 epochs\n",
      "Finished reading the train set of COVID19 Dataset (1800 samples found, each dim = 14)\n",
      "Finished reading the dev set of COVID19 Dataset (200 samples found, each dim = 14)\n",
      "Epoch 00789: reducing learning rate of group 0 to 3.1444e-04.\n",
      "Epoch 00862: reducing learning rate of group 0 to 1.5722e-04.\n",
      "Epoch 00913: reducing learning rate of group 0 to 7.8609e-05.\n",
      "Epoch 01004: reducing learning rate of group 0 to 3.9305e-05.\n",
      "Epoch 01055: reducing learning rate of group 0 to 1.9652e-05.\n",
      "Epoch 01106: reducing learning rate of group 0 to 9.8262e-06.\n",
      "Epoch 01157: reducing learning rate of group 0 to 4.9131e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-29 22:16:57,169] Trial 19 finished with value: 0.7467766726016998 and parameters: {'lr': 0.0006288757158648911, 'batch_size': 164, 'n_epochs': 5940, 'early_stop': 261, 'n_neurons': 86, 'optimizer': 'Adam'}. Best is trial 11 with value: 0.7402272905409336.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01208: reducing learning rate of group 0 to 2.4565e-06.\n",
      "Finished training after 1215 epochs\n",
      "Best hyperparameters:  {'lr': 0.0005673032930370957, 'batch_size': 59, 'n_epochs': 3538, 'early_stop': 336, 'n_neurons': 79, 'optimizer': 'Adam'}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "myseed = 666\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(myseed)\n",
    "torch.manual_seed(myseed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(myseed)\n",
    "    print(\"Cuda Training\")\n",
    "\n",
    "# Get device\n",
    "def get_device():\n",
    "    return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Data processing class\n",
    "class Dataprocess(Dataset):\n",
    "    def __init__(self, path, mode='train', modify=True):\n",
    "        self.mode = mode\n",
    "        with open(path, 'r') as f:\n",
    "            data = list(csv.reader(f))\n",
    "            data = np.array(data[1:])[:, 1:].astype(float)\n",
    "        if modify == False:\n",
    "            feats = list(range(0, 93))\n",
    "        else:\n",
    "            feats_symp1 = list(range(40,44))    # day1 symptom like Covid-19\n",
    "            feats_testp1 = [57]                 # day2 tested_positive\n",
    "            feats_symp2 = list(range(58,62))    # day2 symptom like Covid-19\n",
    "            feats_testp2 = [75]                 # day2 tested_positive \n",
    "            feats_symp3 = list(range(76,80))    # day3 symptom like Covid-19\n",
    "\n",
    "            feats = feats_symp1 + feats_testp1 + feats_symp2 + feats_testp2 + feats_symp3\n",
    "        if mode == 'test':\n",
    "            feats = [f + 1 for f in feats]\n",
    "            data = data[:, feats]\n",
    "            self.data = torch.FloatTensor(data)\n",
    "        else:\n",
    "            target = data[:, -1]\n",
    "            data = data[:, feats]\n",
    "            if mode == 'train':\n",
    "                indices = [i for i in range(len(data)) if i % 10 != 0]\n",
    "            elif mode == 'dev':\n",
    "                indices = [i for i in range(len(data)) if i % 10 == 0]\n",
    "            self.data = torch.FloatTensor(data[indices])\n",
    "            self.target = torch.FloatTensor(target[indices])\n",
    "        self.data[:, 40:] = (self.data[:, 40:] - self.data[:, 40:].mean(dim=0, keepdim=True)) / self.data[:, 40:].std(dim=0, keepdim=True)\n",
    "        self.dim = self.data.shape[1]\n",
    "        print('Finished reading the {} set of COVID19 Dataset ({} samples found, each dim = {})'.format(mode, len(self.data), self.dim))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.mode in ['train', 'dev']:\n",
    "            return self.data[index], self.target[index]\n",
    "        else:\n",
    "            return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "# Data loader\n",
    "def dataloader(path, mode, batch_size, n_jobs=0, modify=False):\n",
    "    dataset = Dataprocess(path, mode=mode, modify=modify)\n",
    "    dataloader = DataLoader(dataset, batch_size, shuffle=(mode == 'train'), drop_last=False, num_workers=n_jobs, pin_memory=True)\n",
    "    return dataloader\n",
    "\n",
    "# Neural network model\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_dim, n_layers, n_neurons):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        \n",
    "        # Try to modify this DNN to achieve better performance\n",
    "        # xby_revised\n",
    "        # BN + Dropout\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, n_neurons),\n",
    "            nn.BatchNorm1d(n_neurons),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=0.35),\n",
    "            nn.Linear(n_neurons, 1)\n",
    "        )\n",
    "\n",
    "        # Loss function MSE\n",
    "        self.criterion = nn.MSELoss(reduction='mean')\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(1)\n",
    "\n",
    "    def cal_loss(self, pred, target):\n",
    "        return self.criterion(pred, target)\n",
    "\n",
    "# Training function\n",
    "def train(tr_set, dv_set, model, config, device):\n",
    "    n_epochs = config['n_epochs']\n",
    "    optimizer = getattr(torch.optim, config['optimizer'])(model.parameters(), **config['optim_hparas'])\n",
    "    # xby_revised\n",
    "    # learning rate scheduler \n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=50, verbose=False)\n",
    "    min_mse = 1000.\n",
    "    loss_record = {'train': [], 'dev': []}\n",
    "    early_stop_cnt = 0\n",
    "    epoch = 0\n",
    "    while epoch < n_epochs:\n",
    "        model.train()\n",
    "        for x, y in tr_set:\n",
    "            optimizer.zero_grad()\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pred = model(x)\n",
    "            mse_loss = model.cal_loss(pred, y)\n",
    "            mse_loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_record['train'].append(mse_loss.detach().cpu().item())\n",
    "        dev_mse = dev(dv_set, model, device)\n",
    "        if dev_mse < min_mse:\n",
    "            min_mse = dev_mse\n",
    "            # print('Saving model (epoch = {:4d}, loss = {:.4f})'.format(epoch + 1, min_mse))\n",
    "            torch.save(model.state_dict(), config['save_path'])\n",
    "            early_stop_cnt = 0\n",
    "        else:\n",
    "            early_stop_cnt += 1\n",
    "        epoch += 1\n",
    "\n",
    "        # xby revised\n",
    "        # scheduler\n",
    "        scheduler.step(dev_mse)\n",
    "\n",
    "        loss_record['dev'].append(dev_mse)\n",
    "        if early_stop_cnt > config['early_stop']:\n",
    "            break\n",
    "    print('Finished training after {} epochs'.format(epoch))\n",
    "    return min_mse, loss_record\n",
    "\n",
    "# Validation function\n",
    "def dev(dv_set, model, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for x, y in dv_set:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = model(x)\n",
    "            mse_loss = model.cal_loss(pred, y)\n",
    "        total_loss += mse_loss.detach().cpu().item() * len(x)\n",
    "    total_loss = total_loss / len(dv_set.dataset)\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "# Main program\n",
    "device = torch.device('cpu')\n",
    "os.makedirs('models', exist_ok=True)\n",
    "modify = True\n",
    "\n",
    "\n",
    "# train_path = './HW/HW1/HW1.train.csv'  # path to training data\n",
    "# test_path = './HW/HW1/HW1.test.csv'   # path to testing data\n",
    "train_path = './HW1/HW1.train.csv'  # path to training data\n",
    "test_path = './HW1/HW1.test.csv'   # path to testing data\n",
    "\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameter search space\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_int('batch_size', 32, 256, log=True)\n",
    "    n_epochs = trial.suggest_int('n_epochs', 1000, 6000,log=True)\n",
    "    early_stop = trial.suggest_int('early_stop', 100, 500,log=True)\n",
    "    # n_layers = trial.suggest_int('n_layers', 1, 1)\n",
    "\n",
    "    n_neurons = trial.suggest_int('n_neurons', 50, 300, log=True)\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD'])\n",
    "\n",
    "    # Define data loaders\n",
    "    train_set = dataloader(train_path, 'train', batch_size, modify=modify)\n",
    "    val_set = dataloader(train_path, 'dev', batch_size, modify=modify)\n",
    "    \n",
    "    # Define model\n",
    "    model = NeuralNet(train_set.dataset.dim, 1, n_neurons).to(device)\n",
    "\n",
    "    config = {\n",
    "    'n_epochs': n_epochs,                # maximum number of epochs\n",
    "    'batch_size': batch_size,               # mini-batch size for dataloader\n",
    "    'optimizer': optimizer_name,              # optimization algorithm (optimizer in torch.optim)\n",
    "    'optim_hparas': {                # hyper-parameters for the optimizer (depends on which optimizer you are using)\n",
    "        'lr': lr,                 # learning rate of SGD\n",
    "    },\n",
    "    'early_stop': early_stop,               # early stopping epochs (the number epochs since your model's last improvement)\n",
    "    'save_path': './HW1/models/model_optuna.pth'           # your model will be saved here\n",
    "}\n",
    "\n",
    "    model_loss, model_loss_record = train(train_set, val_set, model, config, device)\n",
    "\n",
    "    return model_loss\n",
    "\n",
    "# Create Optuna study\n",
    "study = optuna.create_study(direction='minimize')\n",
    "# Run optimization\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# Output best hyperparameters\n",
    "print('Best hyperparameters: ', study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a410361-6802-4f13-970a-da672c151140",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "8007",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
