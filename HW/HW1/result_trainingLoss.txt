

Base-L2_0-40+41-44+59-62+77-80_0928-1044
	epochs: 20000
	batch_size: 64
	optimizer: SGD
	learning_rates: {'lr': 0.001}
	early_stop1000
Training Loss: 1.3229917693138122

Base-L2_0-40+41-44+59-62+77-80_0928-1059
	epochs: 20000
	batch_size: 128
	optimizer: SGD
	learning_rates: {'lr': 0.001}
	early_stop1000
Training Loss: 1.5214102125167848

Base-L2_0-40+41-44+59-62+77-80_0928-1112
	epochs: 20000
	batch_size: 128
	optimizer: SGD
	learning_rates: {'lr': 0.001}
	early_stop1000
Training Loss: 1.5214102125167848

Base_states+symptom+testP_0928-1655
	epochs: 20000
	batch_size: 256
	optimizer: SGD
	learning_rates: {'lr': 0.001}
	early_stop1000
Training Loss: 0.7676059007644653

Base_states+symptom+testP_0928-1705
	epochs: 20000
	batch_size: 256
	optimizer: SGD
	learning_rates: {'lr': 0.001}
	early_stop300
Training Loss: 0.7709358930587769

Base_states+symptom+testP_0928-1709
	epochs: 20000
	batch_size: 1024
	optimizer: SGD
	learning_rates: {'lr': 0.001}
	early_stop300
Training Loss: 0.7760742902755737

ADAM-ReduceLROnPlateau_states-symptom-testP_0928-2035
	epochs: 5000
	batch_size: 64
	optimizer: Adam
	learning_rates: {'lr': 0.001}
	early_stop: 500
Training Loss: 0.923818187713623

ADAM-ReduceLROnPlateau_symptom-testP_0928-2119
	epochs: 5000
	batch_size: 64
	optimizer: Adam
	learning_rates: {'lr': 0.001}
	early_stop: 500
Training Loss: 0.8544239282608033

ADAM-ReduceLROnPlateau_symptom-testP_0928-2122
	epochs: 5000
	batch_size: 256
	optimizer: Adam
	learning_rates: {'lr': 0.001}
	early_stop: 500
Training Loss: 0.8621336817741394

ADAM-ReduceLROnPlateau_symptom-testP_0929-0837
	epochs: 5000
	batch_size: 256
	optimizer: Adam
	learning_rates: {'lr': 0.001}
	early_stop: 500
Training Loss: 0.8621336817741394

ADAM-ReduceLROnPlateau_symptom-testP_0929-0839
	epochs: 5000
	batch_size: 512
	optimizer: Adam
	learning_rates: {'lr': 0.001}
	early_stop: 500
Training Loss: 0.8602687120437622

ADAM-ReduceLROnPlateau_symptom-testP_0929-2225
	epochs: 3538
	batch_size: 59
	optimizer: Adam
	learning_rates: {'lr': 0.0005673032930370957}
	early_stop: 336
Training Loss: 0.7552018935978413

ADAM-ReduceLROnPlateau_Pearson_Optuna1001-0823
	epochs: 1655
	batch_size: 97
	optimizer: Adam
	learning_rates: {'lr': 0.0009132534507097574}
	early_stop: 317
Training Loss: 0.7398897141218186

ADAM-ReduceLROnPlateau_Pearson_Optuna_1001-0831
	epochs: 2000
	batch_size: 128
	optimizer: Adam
	learning_rates: {'lr': 0.0009}
	early_stop: 300
Training Loss: 0.749691195487976

ADAM-CosineAnnealingWarmRestarts_Pearson_1001-0921
	epochs: 2000
	batch_size: 128
	optimizer: Adam
	learning_rates: {'lr': 0.0009}
	early_stop: 300
Training Loss: 0.8128367233276367

ADAM-CosineAnnealingWarmRestarts_Pearson_1001-0924
	epochs: 2000
	batch_size: 128
	optimizer: Adam
	learning_rates: {'lr': 0.0009}
	early_stop: 300
Training Loss: 0.7553051853179932

